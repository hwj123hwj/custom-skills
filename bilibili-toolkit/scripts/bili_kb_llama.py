"""
Bç«™è§†é¢‘çŸ¥è¯†åº“æ„å»ºå·¥å…·
åŠŸèƒ½: å°†æ•°æ®åº“ä¸­çš„è§†é¢‘æ–‡ç¨¿è½¬æ¢ä¸ºå‘é‡ç´¢å¼•,æ”¯æŒè¯­ä¹‰æœç´¢
"""

# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "llama-index",
#     "llama-index-vector-stores-postgres",
#     "llama-index-llms-openai-like",
#     "llama-index-embeddings-openai",
#     "python-dotenv",
#     "SQLAlchemy",
#     "psycopg[binary]",
#     "httpx",
#     "nest_asyncio",
#     "rich",
# ]
# ///

import os
import sys
import asyncio
import httpx
import argparse
from datetime import datetime, timedelta
from typing import List, Optional, Set
from dotenv import load_dotenv
import nest_asyncio
import json
from sqlalchemy import bindparam, create_engine, text
from sqlalchemy.engine import URL
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TaskProgressColumn
from rich.panel import Panel

console = Console()

# ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜
if sys.platform == "win32":
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

nest_asyncio.apply()
from llama_index.core import Document, StorageContext, VectorStoreIndex, Settings, load_index_from_storage
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.node_parser import SentenceSplitter
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.llms.openai_like import OpenAILike

load_dotenv()

# ================= è‡ªå®šä¹‰ OpenAI å…¼å®¹ Embedding ç±» =================
class SiliconFlowEmbedding(BaseEmbedding):
    """é€‚é…ç¡…åŸºæµåŠ¨ç­‰ OpenAI å…¼å®¹æ¥å£çš„é€šç”¨ Embedding ç±»"""
    model_name: str
    api_key: str
    api_base: str

    def __init__(self, model_name: str, api_key: str, api_base: str, **kwargs):
        super().__init__(model_name=model_name, api_key=api_key, api_base=api_base, **kwargs)

    @classmethod
    def class_name(cls) -> str:
        return "SiliconFlowEmbedding"

    def _get_query_embedding(self, query: str) -> List[float]:
        return asyncio.run(self._aget_query_embedding(query))

    def _get_text_embedding(self, text: str) -> List[float]:
        return asyncio.run(self._aget_text_embedding(text))

    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        return asyncio.run(self._aget_text_embeddings(texts))

    async def _aget_query_embedding(self, query: str) -> List[float]:
        embeddings = await self._aget_text_embeddings([query])
        return embeddings[0]

    async def _aget_text_embedding(self, text: str) -> List[float]:
        embeddings = await self._aget_text_embeddings([text])
        return embeddings[0]

    async def _aget_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        # æ‰‹åŠ¨é™åˆ¶æ¯æ‰¹å¤§å°,é˜²æ­¢ API æŠ¥ 413 é”™è¯¯
        max_batch = 4
        all_embeddings = []

        url = f"{self.api_base}/embeddings"
        headers = {"Authorization": f"Bearer {self.api_key}"}

        async with httpx.AsyncClient() as client:
            for i in range(0, len(texts), max_batch):
                batch_text = texts[i:i+max_batch]
                payload = {"model": self.model_name, "input": batch_text}

                response = await client.post(url, json=payload, headers=headers, timeout=60)
                if response.status_code == 413:
                    print(f"âš ï¸ æ‰¹æ¬¡ä»è¿‡å¤§ ({len(batch_text)}),å°è¯•å•æ¡å‘é€...")
                    # è¿›ä¸€æ­¥æ‹†åˆ†åˆ° 1 æ¡
                    for single_text in batch_text:
                        r = await client.post(url, json={"model": self.model_name, "input": [single_text]}, headers=headers)
                        r.raise_for_status()
                        all_embeddings.append(r.json()["data"][0]["embedding"])
                else:
                    response.raise_for_status()
                    data = response.json()
                    all_embeddings.extend([item["embedding"] for item in data["data"]])

        return all_embeddings

# ================= é…ç½®åŒº =================
# ================= ç¯å¢ƒå˜é‡å¢å¼ºåŠ è½½ =================
def load_secrets():
    """é€’å½’å‘ä¸ŠæŸ¥æ‰¾ secrets.json"""
    import json
    current_dir = os.path.dirname(os.path.abspath(__file__))
    while True:
        secrets_path = os.path.join(current_dir, "secrets.json")
        if os.path.exists(secrets_path):
            try:
                with open(secrets_path, "r", encoding="utf-8") as f:
                    return json.load(f)
            except Exception:
                return {}
        
        parent_dir = os.path.dirname(current_dir)
        if parent_dir == current_dir:  # åˆ°è¾¾æ ¹ç›®å½•
            return {}
        current_dir = parent_dir

SECRETS = load_secrets()

def get_env_flexible(key_name, default=None):
    """ä¼˜å…ˆä» os.getenv è·å–ï¼Œå¦‚æœä¸ºç©ºåˆ™ Windows æ³¨å†Œè¡¨è¯»å–ï¼Œæœ€å secrets.json"""
    val = os.getenv(key_name)
    if val: return val
    
    if sys.platform == "win32":
        try:
            import winreg
            with winreg.OpenKey(winreg.HKEY_CURRENT_USER, r"Environment") as key:
                val, _ = winreg.QueryValueEx(key, key_name)
                if val: return val
        except Exception:
            pass
            
    if SECRETS and key_name in SECRETS:
        return SECRETS[key_name]
    return default

SILICONFLOW_API_KEY = get_env_flexible("SILICONFLOW_API_KEY")
SILICONFLOW_BASE_URL = "https://api.siliconflow.cn/v1"
EMBED_MODEL_NAME = "BAAI/bge-m3"

LONGMAO_API_KEY = get_env_flexible("LONGMAO_API_KEY")
LONGMAO_BASE_URL = get_env_flexible("LONGMAO_BASE_URL")
LLM_MODEL_NAME = get_env_flexible("LONGMAO_MODEL", "LongCat-Flash-Chat")

# è®¾ç½®å…¨å±€ LlamaIndex é…ç½®
Settings.embed_model = SiliconFlowEmbedding(
    model_name=EMBED_MODEL_NAME,
    api_key=SILICONFLOW_API_KEY,
    api_base=SILICONFLOW_BASE_URL,
)
Settings.embed_batch_size = 10  # æé«˜æ‰¹å¤„ç†å¤§å°ä»¥æå‡æ€§èƒ½
Settings.llm = OpenAILike(
    model=LLM_MODEL_NAME,
    api_key=LONGMAO_API_KEY,
    api_base=LONGMAO_BASE_URL,
    is_chat_model=True,
)
# é…ç½®æ–‡æœ¬åˆ†å—å™¨ï¼šå°†é•¿æ–‡æœ¬åˆ‡åˆ†ä¸ºé€‚å½“å¤§å°çš„å—
Settings.node_parser = SentenceSplitter(
    chunk_size=512,  # æ¯å—512å­—ç¬¦
    chunk_overlap=50,  # å—ä¹‹é—´50å­—ç¬¦é‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    paragraph_separator="\n\n",
)

# ================= æ•°æ®åº“æ“ä½œå‡½æ•° =================

_ENGINE = None

def get_db_config():
    """ä»ç¯å¢ƒå˜é‡æˆ– secrets.json è·å–æ•°æ®åº“é…ç½®"""
    return {
        "dbname": get_env_flexible("DB_NAME", "media_knowledge_base"),
        "user": get_env_flexible("DB_USER", "root"),
        "password": get_env_flexible("DB_PASSWORD", "15671040800q"),
        "host": get_env_flexible("DB_HOST", "127.0.0.1"),
        "port": get_env_flexible("DB_PORT", "5433")
    }

def get_engine():
    global _ENGINE
    if _ENGINE is not None:
        return _ENGINE

    config = get_db_config()
    port = config.get("port")
    try:
        port = int(port) if port is not None else None
    except (TypeError, ValueError):
        port = None

    _ENGINE = create_engine(
        URL.create(
            "postgresql+psycopg",
            username=config.get("user"),
            password=config.get("password"),
            host=config.get("host"),
            port=port,
            database=config.get("dbname"),
        ),
        pool_pre_ping=True,
    )
    return _ENGINE

def get_indexed_bvids() -> Set[str]:
    """è·å–å·²ç´¢å¼•çš„ BVID é›†åˆ"""
    try:
        with get_engine().connect() as conn:
            rows = conn.execute(
                text(
                    "SELECT DISTINCT metadata_->>'bvid' FROM data_llama_collection WHERE metadata_->>'bvid' IS NOT NULL"
                )
            ).fetchall()
        return {row[0] for row in rows if row[0]}
    except Exception as e:
        print(f"âš ï¸ è·å–å·²ç´¢å¼•åˆ—è¡¨å¤±è´¥: {e}")
        return set()

def get_videos_from_db(up_mid: Optional[int] = None, days: Optional[int] = None,
                      bvids: Optional[List[str]] = None) -> List[tuple]:
    """
    ä»æ•°æ®åº“è·å–è§†é¢‘åˆ—è¡¨

    Args:
        up_mid: UPä¸»ID,åªè·å–è¯¥UPä¸»çš„è§†é¢‘
        days: å¤©æ•°,åªè·å–æœ€è¿‘Nå¤©çš„è§†é¢‘
        bvids: BVIDåˆ—è¡¨,åªè·å–æŒ‡å®šçš„è§†é¢‘

    Returns:
        List of (bvid, title, content_text) tuples
    """
    # æ„å»ºæŸ¥è¯¢æ¡ä»¶
    conditions = ["content_text IS NOT NULL"]
    params: dict = {}

    if up_mid:
        conditions.append("up_mid = :up_mid")
        params["up_mid"] = up_mid

    if days:
        date_threshold = datetime.now() - timedelta(days=days)
        conditions.append("pub_time >= :date_threshold")
        params["date_threshold"] = date_threshold

    if bvids:
        conditions.append("bvid IN :bvids")
        params["bvids"] = bvids

    sql = f"""
        SELECT bvid, title, content_text, up_mid
        FROM bili_video_contents
        WHERE {' AND '.join(conditions)}
        ORDER BY pub_time DESC
    """

    stmt = text(sql)
    if bvids:
        stmt = stmt.bindparams(bindparam("bvids", expanding=True))

    with get_engine().connect() as conn:
        return conn.execute(stmt, params).fetchall()

def get_index_stats() -> dict:
    """è·å–ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
    stats = {
        "total_videos": 0,
        "indexed_videos": 0,
        "total_docs": 0,
        "index_size_mb": 0,
    }

    try:
        with get_engine().connect() as conn:
            # è·å–æ•°æ®åº“ä¸­æœ‰æ–‡ç¨¿çš„è§†é¢‘æ€»æ•°
            stats["total_videos"] = conn.execute(
                text("SELECT COUNT(*) FROM bili_video_contents WHERE content_text IS NOT NULL")
            ).scalar_one()

            # è·å–å·²ç´¢å¼•çš„è§†é¢‘æ•°ï¼ˆé€šè¿‡ metadata ä¸­çš„ bvid ç»Ÿè®¡ï¼‰
            stats["indexed_videos"] = conn.execute(
                text(
                    "SELECT COUNT(DISTINCT metadata_->>'bvid') FROM data_llama_collection WHERE metadata_->>'bvid' IS NOT NULL"
                )
            ).scalar_one()

            # è·å–æ€»æ–‡æ¡£æ•°(å¯èƒ½ä¸€ä¸ªè§†é¢‘è¢«æ‹†åˆ†)
            stats["total_docs"] = conn.execute(text("SELECT COUNT(*) FROM data_llama_collection")).scalar_one()

            # è·å–è¡¨å¤§å°(MB)
            size_str = conn.execute(
                text("SELECT pg_size_pretty(pg_total_relation_size('data_llama_collection')) as size")
            ).scalar_one_or_none()
        # è§£æ size å­—ç¬¦ä¸² (å¦‚ "1234 MB")
        try:
            if size_str:
                stats["index_size_mb"] = float(size_str.split()[0])
        except:
            pass
    except Exception as e:
        print(f"âš ï¸ è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")

    return stats

def clear_index():
    """æ¸…ç©ºå‘é‡ç´¢å¼•"""
    try:
        with get_engine().begin() as conn:
            result = conn.execute(text("DELETE FROM data_llama_collection"))
            deleted = result.rowcount or 0
        print(f"âœ… å·²æ¸…ç©ºç´¢å¼•,åˆ é™¤ {deleted} æ¡è®°å½•")
        return True
    except Exception as e:
        print(f"âŒ æ¸…ç©ºç´¢å¼•å¤±è´¥: {e}")
        return False

def delete_from_index(bvids: List[str]):
    """ä»ç´¢å¼•ä¸­åˆ é™¤æŒ‡å®šè§†é¢‘"""
    try:
        stmt = text("DELETE FROM data_llama_collection WHERE metadata_->>'bvid' IN :bvids").bindparams(
            bindparam("bvids", expanding=True)
        )
        with get_engine().begin() as conn:
            result = conn.execute(stmt, {"bvids": bvids})
            deleted = result.rowcount or 0
        print(f"âœ… å·²ä»ç´¢å¼•ä¸­åˆ é™¤ {deleted} æ¡è®°å½•")
        return True
    except Exception as e:
        print(f"âŒ åˆ é™¤ç´¢å¼•å¤±è´¥: {e}")
        return False

# ================= ç´¢å¼•æ„å»ºå‡½æ•° =================

async def build_index(up_mid: Optional[int] = None, days: Optional[int] = None,
                    bvids: Optional[List[str]] = None, force_rebuild: bool = False):
    """æ„å»ºå‘é‡ç´¢å¼•
    """
    config = get_db_config()

    # 1. è·å–è¦ç´¢å¼•çš„è§†é¢‘åˆ—è¡¨
    print("ğŸ“¥ æ­£åœ¨ä»æ•°æ®åº“è¯»å–è§†é¢‘åˆ—è¡¨...")
    rows = get_videos_from_db(up_mid, days, bvids)

    if not rows:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è§†é¢‘")
        return

    print(f"ğŸ“Š æ‰¾åˆ° {len(rows)} ä¸ªè§†é¢‘")

    # 2. è·å–å·²ç´¢å¼•çš„ BVID (å¢é‡æ›´æ–°)
    indexed_bvids = set()
    if not force_rebuild:
        indexed_bvids = get_indexed_bvids()
        print(f"âœ… å·²å­˜åœ¨ {len(indexed_bvids)} ä¸ªè§†é¢‘çš„ç´¢å¼•")

    # 3. è¿‡æ»¤å‡ºéœ€è¦ç´¢å¼•çš„è§†é¢‘
    videos_to_index = []
    skipped = 0

    for bvid, title, content, up_mid in rows:
        if not force_rebuild and bvid in indexed_bvids:
            skipped += 1
        else:
            videos_to_index.append((bvid, title, content, up_mid))

    if not videos_to_index:
        print(f"âœ… æ‰€æœ‰è§†é¢‘å·²ç´¢å¼•,è·³è¿‡ {skipped} ä¸ª")
        return

    print(f"ğŸ”§ å‡†å¤‡ç´¢å¼• {len(videos_to_index)} ä¸ªæ–°è§†é¢‘ (è·³è¿‡ {skipped} ä¸ªå·²å­˜åœ¨)")

    # 4. åˆå§‹åŒ– PGVectorStore
    vector_store = PGVectorStore.from_params(
        host=config["host"],
        port=config["port"],
        database=config["dbname"],
        user=config["user"],
        password=config["password"],
        table_name="llama_collection",
        embed_dim=1024,
        perform_setup=False,
        hybrid_search=True,
    )

    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    # 5. æ‰¹é‡æ„å»ºæ–‡æ¡£
    console.print("ğŸ“ å‡†å¤‡æ–‡æ¡£...")
    documents = []
    for bvid, title, content, up_mid in videos_to_index:
        doc = Document(
            text=content,
            id_=bvid,
            metadata={
                "bvid": bvid,
                "title": title,
                "up_mid": up_mid or 0,
                "source": "bilibili"
            }
        )
        documents.append(doc)

    # 6. æ‰¹é‡ç´¢å¼•ï¼ˆè‡ªåŠ¨åˆ†å—ï¼‰
    success_count = 0
    fail_count = 0
    start_time = datetime.now()

    try:
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            console=console
        ) as progress:
            task = progress.add_task("[cyan]æ­£åœ¨æ„å»ºç´¢å¼•...", total=len(documents))
            
            index = VectorStoreIndex.from_documents(
                documents, 
                storage_context=storage_context, 
                show_progress=False
            )
            progress.update(task, advance=len(documents))
            
        success_count = len(documents)
        console.print("[bold green]âœ… æ‰¹é‡ç´¢å¼•å®Œæˆ[/bold green]")
    except Exception as e:
        print(f"âŒ æ‰¹é‡ç´¢å¼•å¤±è´¥: {e}")
        print("âš ï¸ å°è¯•é€ä¸ªç´¢å¼•...")
        # å¦‚æœæ‰¹é‡å¤±è´¥ï¼Œé™çº§ä¸ºé€ä¸ªç´¢å¼•
        for i, doc in enumerate(documents, 1):
            try:
                title = doc.metadata.get('title', 'Unknown')
                print(f"[{i}/{len(documents)}] ç´¢å¼•: {title}")
                temp_index = VectorStoreIndex.from_documents(
                    [doc],
                    storage_context=storage_context,
                    show_progress=False
                )
                success_count += 1
            except Exception as e:
                print(f"âŒ ç´¢å¼•å¤±è´¥ {doc.id_}: {e}")
                fail_count += 1

    # 7. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    elapsed = (datetime.now() - start_time).total_seconds()
    print("\n" + "=" * 60)
    print("ğŸ“ˆ ç´¢å¼•æ„å»ºå®Œæˆ!")
    print(f"  âœ… æˆåŠŸ: {success_count}")
    print(f"  âŒ å¤±è´¥: {fail_count}")
    print(f"  â­ï¸  è·³è¿‡: {skipped}")
    print(f"  â±ï¸  è€—æ—¶: {elapsed:.1f}ç§’")
    if success_count > 0:
        print(f"  ğŸ“Š å¹³å‡: {elapsed/success_count:.1f}ç§’/è§†é¢‘")
    print("=" * 60)

    # 8. æ˜¾ç¤ºæ›´æ–°åçš„ç´¢å¼•çŠ¶æ€
    print("\nğŸ“Š å½“å‰ç´¢å¼•çŠ¶æ€:")
    show_stats()

def show_stats():
    """æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯"""
    stats = get_index_stats()

    print(f"  ğŸ“¹ æ•°æ®åº“è§†é¢‘æ€»æ•°: {stats['total_videos']}")
    print(f"  âœ… å·²ç´¢å¼•è§†é¢‘æ•°: {stats['indexed_videos']}")
    print(f"  ğŸ“„ æ€»æ–‡æ¡£æ•°: {stats['total_docs']}")
    if stats['index_size_mb'] > 0:
        print(f"  ğŸ’¾ ç´¢å¼•å¤§å°: {stats['index_size_mb']:.1f} MB")

    # è®¡ç®—è¦†ç›–ç‡
    if stats['total_videos'] > 0:
        coverage = (stats['indexed_videos'] / stats['total_videos']) * 100
        print(f"  ğŸ“Š è¦†ç›–ç‡: {coverage:.1f}%")

def validate_index():
    """éªŒè¯ç´¢å¼•æ˜¯å¦æ­£å¸¸å·¥ä½œ"""
    try:
        print("\nğŸ” éªŒè¯ç´¢å¼•...")
        config = get_db_config()

        # åˆå§‹åŒ–å‘é‡å­˜å‚¨
        vector_store = PGVectorStore.from_params(
            host=config["host"],
            port=config["port"],
            database=config["dbname"],
            user=config["user"],
            password=config["password"],
            table_name="data_llama_collection",
            embed_dim=1024,
        )

        storage_context = StorageContext.from_defaults(vector_store=vector_store)
        index = VectorStoreIndex.from_vector_store(vector_store, storage_context=storage_context)

        # æµ‹è¯•æŸ¥è¯¢
        query_engine = index.as_query_engine(similarity_top_k=1)
        response = query_engine.query("æµ‹è¯•")
        print(f"âœ… ç´¢å¼•éªŒè¯æˆåŠŸ")

        return True
    except Exception as e:
        print(f"âŒ ç´¢å¼•éªŒè¯å¤±è´¥: {e}")
        return False

# ================= å‘½ä»¤è¡Œæ¥å£ =================

def main():
    parser = argparse.ArgumentParser(
        description="Bç«™è§†é¢‘çŸ¥è¯†åº“æ„å»ºå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å…¨é‡æ„å»º(å¢é‡æ¨¡å¼)
  %(prog)s

  # æŸ¥çœ‹ç´¢å¼•çŠ¶æ€
  %(prog)s --stats

  # å¼ºåˆ¶é‡å»ºæ‰€æœ‰ç´¢å¼•
  %(prog)s --rebuild

  # åªç´¢å¼•æŒ‡å®šUPä¸»çš„è§†é¢‘
  %(prog)s --up 3546830417693175

  # åªç´¢å¼•æœ€è¿‘30å¤©çš„è§†é¢‘
  %(prog)s --days 30

  # åªç´¢å¼•æŒ‡å®šçš„è§†é¢‘
  %(prog)s --bvids BV1xx411c7mD BV1yy411c7mE

  # ç»„åˆæ¡ä»¶
  %(prog)s --up 3546830417693175 --days 7

  # åˆ é™¤æŒ‡å®šè§†é¢‘çš„ç´¢å¼•
  %(prog)s --delete BV1xx411c7mD
        """
    )

    parser.add_argument("--stats", action="store_true", help="æ˜¾ç¤ºç´¢å¼•ç»Ÿè®¡ä¿¡æ¯")
    parser.add_argument("--rebuild", action="store_true", help="æ¸…ç©ºå¹¶é‡å»ºç´¢å¼•")
    parser.add_argument("--up", type=int, metavar="UID", help="åªç´¢å¼•æŒ‡å®šUPä¸»çš„è§†é¢‘")
    parser.add_argument("--days", type=int, metavar="N", help="åªç´¢å¼•æœ€è¿‘Nå¤©çš„è§†é¢‘")
    parser.add_argument("--bvids", nargs="+", metavar="BVID", help="åªç´¢å¼•æŒ‡å®šçš„è§†é¢‘åˆ—è¡¨")
    parser.add_argument("--delete", nargs="+", metavar="BVID", help="ä»ç´¢å¼•ä¸­åˆ é™¤æŒ‡å®šè§†é¢‘")
    parser.add_argument("--validate", action="store_true", help="éªŒè¯ç´¢å¼•æ˜¯å¦æ­£å¸¸")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡å»º(å¿½ç•¥å·²å­˜åœ¨çš„ç´¢å¼•)")

    args = parser.parse_args()

    # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    if len(sys.argv) == 1:
        parser.print_help()
        return

    # å¤„ç†åˆ é™¤æ“ä½œ
    if args.delete:
        print(f"ğŸ—‘ï¸  åˆ é™¤ç´¢å¼•: {', '.join(args.delete)}")
        delete_from_index(args.delete)
        return

    # å¤„ç†ç»Ÿè®¡ä¿¡æ¯
    if args.stats:
        print("ğŸ“Š ç´¢å¼•ç»Ÿè®¡ä¿¡æ¯:")
        show_stats()
        return

    # å¤„ç†éªŒè¯
    if args.validate:
        show_stats()
        validate_index()
        return

    # å¤„ç†é‡å»º
    if args.rebuild:
        print("ğŸ”„ æ¸…ç©ºå¹¶é‡å»ºç´¢å¼•...")
        if clear_index():
            asyncio.run(build_index(force_rebuild=True))
        return

    # æ„å»ºç´¢å¼•(å¢é‡æˆ–æŒ‡å®šèŒƒå›´)
    asyncio.run(build_index(
        up_mid=args.up,
        days=args.days,
        bvids=args.bvids,
        force_rebuild=args.force
    ))

if __name__ == "__main__":
    main()
